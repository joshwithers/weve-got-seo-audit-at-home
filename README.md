# SEO Audit Engine

**Version 0.2.0** - A local-first SEO website audit tool that runs entirely on your machine. No SaaS, no external dependencies, no cloud services.

## Features

- ğŸ  **Local-First** - All data stays on your machine
- ğŸ“ **Multiple Export Formats** - Markdown, HTML, JSON, CSV
- âœ… **Interactive To-Do Lists** - Track fixes with checkboxes
- ğŸ“Š **SEO Health Score** - 0-100 rating based on issues
- ğŸ¨ **Custom Branding** - Add your business name to reports
- ğŸš« **Smart Filtering** - Automatically skips PDFs, XML, etc.
- ğŸ”§ **CLI-First** - Scriptable and automatable
- ğŸ“¦ **SQLite Storage** - Single file database
- ğŸ”Œ **Modular Checks** - Easy to extend

## Quick Start

### Installation

```bash
# Clone or download the project
cd seo-audit-engine

# Run the install script
./install.sh

# Activate virtual environment
source venv/bin/activate
```

### Run Your First Audit

```bash
# Basic audit (creates Markdown report with to-do list)
audit run https://example.com

# With custom branding
audit run https://example.com \
  --business-name "Your Company" \
  --prepared-by "Your Name"

# Generate HTML report
audit run https://example.com --format html

# All formats at once
audit run https://example.com --format all
```

## What It Does

The tool crawls your website and generates detailed reports with:

### Audit Checks (5 Core Checks)

1. **Broken Links** - Finds 404s and unreachable internal links
2. **Page Titles** - Detects missing, duplicate, too short, or too long titles
3. **Meta Descriptions** - Identifies missing or problematic meta descriptions
4. **Headings** - Checks for missing or multiple H1 tags
5. **Redirects** - Finds redirect chains and loops

### Smart Features

- **File Filtering** - Automatically skips .pdf, .epub, .xml, .txt, .zip files
- **robots.txt Support** - Respects site crawl rules (configurable)
- **URL Normalization** - Handles duplicates intelligently
- **Depth Control** - Configurable crawl depth and page limits
- **Rate Limiting** - Adjustable delay between requests

## Export Formats

### Markdown (Default) - Perfect for LLMs

```bash
audit run https://example.com
# Creates: audit_report.md
```

**Best for:**
- Feeding to ChatGPT/Claude for specific fix instructions
- Tracking progress with GitHub-style checkboxes
- Version control (commit to git)
- Human-readable documentation

**Format includes:**
- Domain-specific title: "SEO Report of example.com"
- Executive summary with SEO Health Score
- Prioritized to-do list (High/Medium/Low priority)
- Issue breakdown by type
- Page inventory table

### HTML - Interactive Browser Report

```bash
audit run https://example.com --format html
# Creates: audit_report.html
```

**Best for:**
- Client presentations
- Visual tracking (checkboxes save state in browser)
- Shareable reports
- Print to PDF

**Features:**
- Color-coded severity indicators (red/yellow/blue)
- Interactive checkboxes with localStorage
- SEO Health Score visualization
- Responsive design

### JSON - Machine Readable

```bash
audit run https://example.com --format json
# Creates: audit_report.json
```

**Best for:**
- API integrations
- Automated processing
- Data pipelines

### CSV - Spreadsheet Analysis

```bash
audit run https://example.com --format csv
# Creates: audit_issues.csv, audit_pages.csv
```

**Best for:**
- Excel/Google Sheets analysis
- Filtering and sorting
- Pivot tables

### All Formats

```bash
audit run https://example.com --format all
# Creates: .md, .html, .json, .csv files
```

## Custom Branding

Add your business name and personal credit to all reports:

```bash
audit run https://example.com \
  --business-name "Josh's SEO Services" \
  --prepared-by "Josh Withers"
```

**Appears in reports as:**
- Header: "SEO Report of example.com"
- Subheader: "Prepared by: Josh Withers"
- Footer: "Report generated by Josh's SEO Services"

## CLI Reference

### Main Commands

```bash
# Run audit
audit run <url> [OPTIONS]

# Export from existing database
audit export [OPTIONS]

# List available checks
audit checks

# Clear database
audit clear

# Show help
audit --help
audit run --help
```

### Options

```bash
--depth N                # Maximum crawl depth (default: 3)
--max-pages N           # Maximum pages to crawl (default: 1000)
--format FORMAT         # Output format: markdown, html, json, csv, all (default: markdown)
--output FILE           # Custom output file path
--business-name NAME    # Business name for report credits
--prepared-by NAME      # Name of person/team preparing report
--delay SECONDS         # Delay between requests (default: 0.5)
--no-robots             # Ignore robots.txt
--db FILE               # Custom database file (default: audit.db)
```

### Examples

```bash
# Deep crawl with custom branding
audit run https://example.com \
  --depth 5 \
  --max-pages 500 \
  --business-name "Your Company" \
  --prepared-by "Your Name"

# HTML report for client
audit run https://clientsite.com \
  --format html \
  --output client_report.html \
  --business-name "Your Agency"

# Export existing audit in different format
audit export --format markdown --prepared-by "John Smith"

# Quick audit (small site)
audit run https://example.com --depth 2 --max-pages 50

# Ignore robots.txt (for owned sites)
audit run https://mysite.com --no-robots

# Slow crawl for production sites
audit run https://example.com --delay 2.0
```

## Using with LLMs

The Markdown format is optimized for feeding to AI assistants:

1. **Run audit:**
   ```bash
   audit run https://yoursite.com
   ```

2. **Copy `audit_report.md` content to Claude/ChatGPT**

3. **Example prompts:**
   ```
   "Based on this SEO audit, generate specific HTML fixes
   for all missing title tags"

   "Create a 2-week action plan to fix these issues,
   prioritized by impact"

   "Generate unique meta descriptions for all pages
   missing them, 120-160 characters each"

   "Write a client-friendly summary of these issues
   and recommended fixes"
   ```

## File Filtering

The crawler automatically skips these file types:
- `.pdf` - PDF documents
- `.epub` - E-books
- `.xml` - XML files (RSS, sitemaps)
- `.txt` - Text files
- `.zip`, `.gz`, `.tar` - Archives

You'll see: `Skipping https://example.com/file.pdf (excluded file type)`

## Architecture

```
audit_engine/
â”œâ”€â”€ cli.py              # CLI interface
â”œâ”€â”€ crawler.py          # Web crawler
â”œâ”€â”€ database.py         # SQLite operations
â”œâ”€â”€ models.py           # Data models
â”œâ”€â”€ exporter.py         # Export functionality
â””â”€â”€ checks/             # Audit checks
    â”œâ”€â”€ base.py
    â”œâ”€â”€ broken_links.py
    â”œâ”€â”€ titles.py
    â”œâ”€â”€ meta_description.py
    â”œâ”€â”€ headings.py
    â””â”€â”€ redirects.py
```

## Requirements

- Python 3.11 or higher
- pip (for installing dependencies)
- Internet connection (for crawling)

## Installation from Source

```bash
# Manual install
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .
```

## Database

All data is stored in SQLite (`audit.db` by default):

- **pages** - Crawled pages and SEO data
- **links** - Internal/external links
- **issues** - Audit findings
- **crawl_meta** - Session metadata

You can query the database directly:
```bash
sqlite3 audit.db "SELECT url, title FROM pages WHERE status_code = 200"
```

## Adding Custom Checks

1. Create a new file in `audit_engine/checks/`
2. Inherit from `BaseCheck`
3. Implement `name`, `description`, and `run()` methods
4. Add to `ALL_CHECKS` in `audit_engine/checks/__init__.py`

Example:
```python
from .base import BaseCheck
from ..models import Issue, Severity

class MyCheck(BaseCheck):
    @property
    def name(self) -> str:
        return "My Custom Check"

    @property
    def description(self) -> str:
        return "Description of what this check does"

    def run(self) -> List[Issue]:
        issues = []
        pages = self.db.get_all_pages()

        for page in pages:
            # Your check logic here
            if condition:
                issues.append(Issue(
                    issue_type="my_issue",
                    severity=Severity.WARNING,
                    description="Issue description",
                    affected_url=page.url
                ))

        return issues
```

## Limitations

By design (MVP scope):
- No JavaScript rendering
- No Core Web Vitals measurement
- No backlink analysis
- Single-threaded crawling
- No external API integrations

## Troubleshooting

### Command Not Found
```bash
# Make sure virtual environment is activated
source venv/bin/activate
```

### Crawl Too Slow
```bash
# Reduce depth or page limit
audit run https://example.com --depth 2 --max-pages 100
```

### Database Locked
```bash
# Clear the database
audit clear
```

## What's New in v0.2.0

- âœ¨ Markdown & HTML export formats
- âœ… Interactive to-do lists with checkboxes
- ğŸ“Š SEO Health Score
- ğŸ¨ Custom branding (business name & prepared by)
- ğŸš« Automatic file filtering (.pdf, .xml, etc.)
- ğŸ“ Reports optimized for LLMs
- ğŸ¯ Prioritized action items

See [CHANGELOG.md](CHANGELOG.md) for full details.

## License

MIT License - See [LICENSE](LICENSE) file

## Support

- **Issues:** Report bugs or request features via GitHub issues
- **Documentation:** This README covers all features
- **Source Code:** Readable, well-commented Python

## Credits

- **Created by:** SEO Audit Engine Team
- **Version:** 0.2.0
- **License:** MIT

---

**Ready to improve your SEO!** ğŸš€

Run `audit run https://yoursite.com` to get started.
